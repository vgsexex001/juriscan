"use client";

import { useState, useRef, useCallback, useEffect } from "react";
import { CHAT_ATTACHMENT_LIMITS } from "@/types/chat";

interface AudioRecorderState {
  isRecording: boolean;
  isPaused: boolean;
  duration: number;
  audioBlob: Blob | null;
  audioUrl: string | null;
  error: string | null;
  isSupported: boolean;
}

interface UseAudioRecorderOptions {
  onWaveformData?: (data: Uint8Array) => void;
  fftSize?: number;
}

interface UseAudioRecorderReturn extends AudioRecorderState {
  startRecording: () => Promise<void>;
  stopRecording: () => void;
  pauseRecording: () => void;
  resumeRecording: () => void;
  resetRecording: () => void;
  cancelRecording: () => void;
  getWaveformData: () => Uint8Array | null;
}

// Get supported MIME type for the browser
function getSupportedMimeType(): string {
  if (typeof window === "undefined") return "audio/webm";

  const types = [
    "audio/webm;codecs=opus",
    "audio/webm",
    "audio/mp4",
    "audio/ogg;codecs=opus",
    "audio/ogg",
  ];

  for (const type of types) {
    if (MediaRecorder.isTypeSupported(type)) {
      console.log("游꿗 Using MIME type:", type);
      return type;
    }
  }

  console.warn("游꿗 No supported MIME type found, defaulting to audio/webm");
  return "audio/webm";
}

export function useAudioRecorder(options: UseAudioRecorderOptions = {}): UseAudioRecorderReturn {
  const { onWaveformData, fftSize = 256 } = options;

  const [state, setState] = useState<AudioRecorderState>({
    isRecording: false,
    isPaused: false,
    duration: 0,
    audioBlob: null,
    audioUrl: null,
    error: null,
    isSupported: false,
  });

  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const chunksRef = useRef<Blob[]>([]);
  const timerRef = useRef<ReturnType<typeof setInterval> | null>(null);
  const startTimeRef = useRef<number>(0);
  const pausedDurationRef = useRef<number>(0);
  const mimeTypeRef = useRef<string>("audio/webm");

  // Audio analysis refs
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const sourceRef = useRef<MediaStreamAudioSourceNode | null>(null);
  const waveformDataRef = useRef<Uint8Array<ArrayBuffer> | null>(null);
  const animationFrameRef = useRef<number | null>(null);

  // Flag to track if recording was cancelled
  const cancelledRef = useRef<boolean>(false);

  // Check support on client
  useEffect(() => {
    const checkSupport = () => {
      const hasMediaDevices = !!navigator?.mediaDevices?.getUserMedia;
      const hasMediaRecorder = typeof MediaRecorder !== "undefined";
      const isSupported = hasMediaDevices && hasMediaRecorder;

      console.log("游꿗 Audio recording support:", {
        hasMediaDevices,
        hasMediaRecorder,
        isSupported,
      });

      if (isSupported) {
        mimeTypeRef.current = getSupportedMimeType();
      }

      setState((prev) => ({ ...prev, isSupported }));
    };

    checkSupport();
  }, []);

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      if (timerRef.current) {
        clearInterval(timerRef.current);
      }
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
      if (streamRef.current) {
        streamRef.current.getTracks().forEach((track) => track.stop());
      }
      if (audioContextRef.current) {
        audioContextRef.current.close();
      }
    };
  }, []);

  const stopTimer = useCallback(() => {
    if (timerRef.current) {
      clearInterval(timerRef.current);
      timerRef.current = null;
    }
  }, []);

  const stopAnalyser = useCallback(() => {
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }
  }, []);

  // Get current waveform data
  const getWaveformData = useCallback((): Uint8Array | null => {
    if (!analyserRef.current || !waveformDataRef.current) return null;
    const data = waveformDataRef.current;
    analyserRef.current.getByteTimeDomainData(data);
    return data;
  }, []);

  // Animation loop for waveform data
  const updateWaveform = useCallback(() => {
    if (analyserRef.current && waveformDataRef.current && onWaveformData) {
      analyserRef.current.getByteTimeDomainData(waveformDataRef.current);
      onWaveformData(waveformDataRef.current);
    }
    animationFrameRef.current = requestAnimationFrame(updateWaveform);
  }, [onWaveformData]);

  const stopRecording = useCallback(() => {
    console.log("游꿗 Stopping recording...");
    stopTimer();
    stopAnalyser();

    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== "inactive") {
      mediaRecorderRef.current.stop();
    }
  }, [stopTimer, stopAnalyser]);

  const startTimer = useCallback(() => {
    startTimeRef.current = Date.now() - pausedDurationRef.current * 1000;

    timerRef.current = setInterval(() => {
      const elapsed = Math.floor((Date.now() - startTimeRef.current) / 1000);

      // Check duration limit
      if (elapsed >= CHAT_ATTACHMENT_LIMITS.maxAudioDuration) {
        console.log("游꿗 Max duration reached, stopping...");
        stopRecording();
        return;
      }

      setState((prev) => ({ ...prev, duration: elapsed }));
    }, 100);
  }, [stopRecording]);

  const startRecording = useCallback(async () => {
    console.log("游꿗 Starting recording...");
    cancelledRef.current = false;

    if (!state.isSupported) {
      console.error("游꿗 Audio recording not supported");
      setState((prev) => ({
        ...prev,
        error: "Grava칞칚o de 치udio n칚o suportada neste navegador",
      }));
      return;
    }

    try {
      // Reset state
      chunksRef.current = [];
      pausedDurationRef.current = 0;

      if (state.audioUrl) {
        URL.revokeObjectURL(state.audioUrl);
      }

      // Request microphone permission
      console.log("游꿗 Requesting microphone permission...");
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          sampleRate: 44100,
        },
      });

      console.log("游꿗 Microphone permission granted, stream:", stream.id);
      streamRef.current = stream;

      // Setup audio analysis for waveform
      try {
        audioContextRef.current = new AudioContext();
        analyserRef.current = audioContextRef.current.createAnalyser();
        analyserRef.current.fftSize = fftSize;
        sourceRef.current = audioContextRef.current.createMediaStreamSource(stream);
        sourceRef.current.connect(analyserRef.current);
        waveformDataRef.current = new Uint8Array(analyserRef.current.frequencyBinCount);

        // Start waveform animation if callback provided
        if (onWaveformData) {
          updateWaveform();
        }
      } catch (audioErr) {
        console.warn("游꿗 Audio analysis not available:", audioErr);
      }

      // Create MediaRecorder with supported MIME type
      const mimeType = mimeTypeRef.current;
      console.log("游꿗 Creating MediaRecorder with mimeType:", mimeType);

      const mediaRecorder = new MediaRecorder(stream, {
        mimeType,
        audioBitsPerSecond: 128000,
      });

      mediaRecorderRef.current = mediaRecorder;

      mediaRecorder.ondataavailable = (event) => {
        console.log("游꿗 Data available:", event.data.size, "bytes");
        if (event.data.size > 0) {
          chunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = () => {
        console.log("游꿗 Recording stopped, chunks:", chunksRef.current.length, "cancelled:", cancelledRef.current);

        // Cleanup audio context
        if (sourceRef.current) {
          sourceRef.current.disconnect();
          sourceRef.current = null;
        }
        if (audioContextRef.current) {
          audioContextRef.current.close();
          audioContextRef.current = null;
        }
        analyserRef.current = null;

        // If cancelled, don't create blob
        if (cancelledRef.current) {
          setState((prev) => ({
            ...prev,
            isRecording: false,
            isPaused: false,
            duration: 0,
            audioBlob: null,
            audioUrl: null,
          }));
          return;
        }

        if (chunksRef.current.length === 0) {
          console.error("游꿗 No audio data collected!");
          setState((prev) => ({
            ...prev,
            isRecording: false,
            error: "Nenhum 치udio foi gravado. Tente novamente.",
          }));
          return;
        }

        const blob = new Blob(chunksRef.current, { type: mimeType });
        const url = URL.createObjectURL(blob);

        console.log("游꿗 Audio blob created:", blob.size, "bytes");

        setState((prev) => ({
          ...prev,
          isRecording: false,
          isPaused: false,
          audioBlob: blob,
          audioUrl: url,
        }));

        // Stop stream
        if (streamRef.current) {
          streamRef.current.getTracks().forEach((track) => track.stop());
          streamRef.current = null;
        }
      };

      mediaRecorder.onerror = (event) => {
        console.error("游꿗 MediaRecorder error:", event);
        stopTimer();
        stopAnalyser();
        setState((prev) => ({
          ...prev,
          isRecording: false,
          error: "Erro ao gravar 치udio. Tente novamente.",
        }));
      };

      // Start recording - request data every 1 second
      mediaRecorder.start(1000);
      startTimer();

      console.log("游꿗 Recording started!");

      setState((prev) => ({
        ...prev,
        isRecording: true,
        isPaused: false,
        duration: 0,
        audioBlob: null,
        audioUrl: null,
        error: null,
      }));
    } catch (error) {
      console.error("游꿗 Error starting recording:", error);

      let errorMessage = "Erro ao iniciar grava칞칚o";
      if (error instanceof Error) {
        console.error("游꿗 Error details:", error.name, error.message);

        if (error.name === "NotAllowedError" || error.name === "PermissionDeniedError") {
          // Check if HTTPS
          const isSecure = typeof window !== "undefined" &&
            (window.location.protocol === "https:" || window.location.hostname === "localhost");

          if (!isSecure) {
            errorMessage = "Grava칞칚o de 치udio requer conex칚o segura (HTTPS).";
          } else {
            errorMessage = "Permiss칚o de microfone negada. Para habilitar:\n1. Clique no 칤cone 游 na barra de endere칞o\n2. Encontre 'Microfone' e selecione 'Permitir'\n3. Recarregue a p치gina";
          }
        } else if (error.name === "NotFoundError") {
          errorMessage = "Nenhum microfone encontrado. Conecte um microfone e tente novamente.";
        } else if (error.name === "NotReadableError") {
          errorMessage = "Microfone est치 sendo usado por outro aplicativo. Feche outros apps e tente novamente.";
        } else if (error.name === "OverconstrainedError") {
          errorMessage = "Configura칞칚o de microfone n칚o suportada.";
        } else if (error.name === "SecurityError") {
          errorMessage = "Erro de seguran칞a. Verifique se o site est치 em HTTPS.";
        } else {
          errorMessage = `Erro ao acessar microfone: ${error.message}`;
        }
      }

      setState((prev) => ({ ...prev, error: errorMessage }));
    }
  }, [state.isSupported, state.audioUrl, fftSize, onWaveformData, startTimer, stopTimer, stopAnalyser, updateWaveform]);

  const pauseRecording = useCallback(() => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === "recording") {
      console.log("游꿗 Pausing recording...");
      mediaRecorderRef.current.pause();
      stopTimer();
      stopAnalyser();
      pausedDurationRef.current = state.duration;
      setState((prev) => ({ ...prev, isPaused: true }));
    }
  }, [state.duration, stopTimer, stopAnalyser]);

  const resumeRecording = useCallback(() => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === "paused") {
      console.log("游꿗 Resuming recording...");
      mediaRecorderRef.current.resume();
      startTimer();
      if (onWaveformData) {
        updateWaveform();
      }
      setState((prev) => ({ ...prev, isPaused: false }));
    }
  }, [startTimer, onWaveformData, updateWaveform]);

  // Cancel recording without saving
  const cancelRecording = useCallback(() => {
    console.log("游꿗 Cancelling recording...");
    cancelledRef.current = true;
    stopTimer();
    stopAnalyser();

    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== "inactive") {
      mediaRecorderRef.current.stop();
    }

    if (streamRef.current) {
      streamRef.current.getTracks().forEach((track) => track.stop());
      streamRef.current = null;
    }

    chunksRef.current = [];
    pausedDurationRef.current = 0;
    mediaRecorderRef.current = null;

    setState((prev) => ({
      ...prev,
      isRecording: false,
      isPaused: false,
      duration: 0,
      audioBlob: null,
      audioUrl: null,
      error: null,
    }));
  }, [stopTimer, stopAnalyser]);

  const resetRecording = useCallback(() => {
    console.log("游꿗 Resetting recording...");
    stopTimer();
    stopAnalyser();

    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== "inactive") {
      mediaRecorderRef.current.stop();
    }

    if (streamRef.current) {
      streamRef.current.getTracks().forEach((track) => track.stop());
      streamRef.current = null;
    }

    if (state.audioUrl) {
      URL.revokeObjectURL(state.audioUrl);
    }

    chunksRef.current = [];
    pausedDurationRef.current = 0;
    mediaRecorderRef.current = null;

    setState((prev) => ({
      ...prev,
      isRecording: false,
      isPaused: false,
      duration: 0,
      audioBlob: null,
      audioUrl: null,
      error: null,
    }));
  }, [state.audioUrl, stopTimer, stopAnalyser]);

  return {
    ...state,
    startRecording,
    stopRecording,
    pauseRecording,
    resumeRecording,
    resetRecording,
    cancelRecording,
    getWaveformData,
  };
}
